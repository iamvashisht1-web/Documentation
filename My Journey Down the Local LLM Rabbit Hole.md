<h1 align="center">üï≥Ô∏èüêáMy Journey Down the Local LLM Rabbit Hole</h1>I've been hearing about local LLMs forever. As a <strong>Business Analyst</strong>, the idea of an AI assistant on my own PC to help draft templates and user stories‚Äîcompletely offline‚Äîwas too good to pass up.I've got a pretty beefy machine, so I figured, "how hard can it be?"<details><summary><strong>üíª View My Specs</strong></summary><ul style="list-style-type: none; margin-left: 10px;"><li><strong>CPU:</strong> Ryzen 9 9900x</li><li><strong>GPU:</strong> RTX 5080 (16GB VRAM)</li></ul></details>Famous last words.‚öôÔ∏è Chapter 1: Ollama - The Developer's GatewayMy first stop was Ollama. Everyone said it was the easiest way to get started. They weren't wrong. A few commands in my terminal, and I was downloading a 16GB Gemma 3 model. It felt like magic.But then I hit a wall. It's... a command-line tool. It's amazing for developers, but I just wanted a simple chat workbench.I also learned that my 16GB model was now "stuck" in Ollama's world. Other apps couldn't see it.<blockquote>üí° <strong>Lesson #1:</strong> All these AI apps are like little walled gardens; they don't share their models.</blockquote>üñ•Ô∏è Chapter 2: LM Studio - The Workbench I WantedThis led me to LM Studio. This was exactly what I was looking for. It's a proper GUI where you can search for models, download them (as .gguf files), and just... chat.I was finally ready to get to work generating my BA and PM templates.Or so I thought. My next problem was speed. I have an RTX 5080. This stuff should be fast.‚ö° Chapter 3: Hitting the "Speed Trap" (And How I Fixed It)At first, things were zippy. But I got greedy.I went into the settings and saw Context Length (the model's short-term memory). I thought, "More is always better!" and cranked it all the way up to 131,072 tokens.The model immediately slowed to a crawl. üê¢ I mean, it was painfully slow.After a lot of frustration, I finally figured it out: By maxing out the context, I was choking my GPU. My 5080 was spending all 16GB of its VRAM just reserving that massive, empty memory block, leaving nothing for the actual processing.That was my big "aha!" moment.My "Go-Fast" ChecklistHere are the settings that made it fly:<table width="100%"><thead><tr><th align="left">Setting</th><th align="left">My Recommendation</th><th align="left">Why (The "Aha!" Moment)</th></tr></thead><tbody><tr><td><strong>‚úÖ GPU Offload</strong></td><td><code>Max</code></td><td>Forces as much of the model as possible onto the GPU's fast VRAM.</td></tr><tr><td><strong>‚ö†Ô∏è Context Length</strong></td><td><code>8192</code> (for 99% of tasks)</td><td><strong>This is the "Speed Trap"!</strong> Maxing it out chokes the GPU. This lower value leaves plenty of VRAM for processing.</td></tr><tr><td><strong>‚úÖ Flash Attention</strong></td><td><code>ON</code></td><td>A free speed boost for modern NVIDIA cards.</td></tr><tr><td><strong>‚úÖ CPU Threads</strong></td><td><code>12</code> (to match my Ryzen 9)</td><td>Balances the load and helps with prompt processing.</td></tr></tbody></table>Suddenly, I had a lightning-fast AI for all my text-based template work.üñºÔ∏è Chapter 4: AnythingLLM & The Multimodal NightmareI got ambitious. I wanted to analyze a bunch of images and have the AI write a compiled document. This led me to AnythingLLM, which is built to "chat" with your documents (or images).This was a whole new level of "why doesn't this work?"First, I had to figure out how to connect LM Studio (as a server) to AnythingLLM.Then I got this error:<blockquote>üö´ "Model does not support images"</blockquote>I learned that I needed a special "Vision" (VL) model.Then, when I tried to upload 9 images, I got this error:<blockquote>üìàüí• "Context overflow: 4096 tokens"</blockquote>It turned out I needed a model that was both a vision model and had a massive context window.üõ†Ô∏è My Final BA ToolkitAfter all that troubleshooting, my local AI setup is now something I use every day. Here's my final toolkit, broken down by task:<table width="100%"><thead><tr><th align="left">Use Case</th><th align="left">Models</th><th align="left">Tools</th><th align="left">Key Setting / Why</th></tr></thead><tbody><tr><td><strong>Fast, Daily Text & Templates</strong><small>(User Stories, Test Cases, Emails)</small></td><td><code>Gemma 3 27B</code> or<code>Llama 3.3 70B</code></td><td><code>LM Studio</code></td><td><strong>Context: <code>8192</code></strong><small>Incredibly fast and smart enough for 90% of my work.</small></td></tr><tr><td><strong>Analyzing Lots of Docs/Images</strong><small>(Multimodal Analysis)</small></td><td><code>Qwen2.5-VL-7B-Instruct-GGUF</code></td><td><code>LM Studio</code> (as server)+ <code>AnythingLLM</code> (as client)</td><td><strong>Context: <code>128k</code></strong><small>It's a "Vision" (VL) model <em>with</em> a huge context to fix all overflow errors.</small></td></tr></tbody></table>It was a journey, but it's amazing what you can get running on your own machine once you figure out which settings to tweak.
